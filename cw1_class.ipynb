{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid_World:\n",
    "    \"\"\"This class constructs the Grid World and transition probabililties and discount factor according to the CID.\n",
    "       It provides th optimal policy and the actual value function states, based on the value iteration algorithm.\"\"\"\n",
    "    \n",
    "    def __init__(self, cid):\n",
    "        \"\"\"This funciton sets-up the grid as well as the probabililtes, discount factor, and relevant matrices\n",
    "           for the computation of the value iteration algorithm. It provides the position of all the unblocked \n",
    "           states of the GridWorld of interest.\n",
    "           input: an (8x1) numpy array with each CID digit.\"\"\"\n",
    "        \n",
    "        self.cid = cid\n",
    "        \n",
    "        self.states_names = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\", \"s6\", \"s7\", \"s8\", \"s9\", \"s10\", \"s11\"]\n",
    "        self.states_pos = np.array([[0, 0], [0, 1], [0, 2], [0, 3], [1, 0], [1, 1], [1, 3], [2, 1], [2, 2], [2, 3], [3, 2]])\n",
    "        self.dic_states = {self.states_names[0]: self.states_pos[0], self.states_names[1]: self.states_pos[1],\\\n",
    "                           self.states_names[2]: self.states_pos[2], self.states_names[3]: self.states_pos[3],\\\n",
    "                           self.states_names[4]: self.states_pos[4], self.states_names[5]: self.states_pos[5],\\\n",
    "                           self.states_names[6]: self.states_pos[6], self.states_names[7]: self.states_pos[7],\\\n",
    "                           self.states_names[8]: self.states_pos[8], self.states_names[9]: self.states_pos[9],\\\n",
    "                           self.states_names[10]: self.states_pos[10]}\n",
    "        self.directions = [\"north\", \"south\", \"east\", \"west\"]\n",
    "        \n",
    "        self.gamma = 0.2 + 0.5*0.1*cid[6] \n",
    "        self.pba = 0.25 + 0.5*0.1*cid[5]\n",
    "        self.pbo = (1-self.pba)/3\n",
    "        self.v = np.zeros(len(self.states_names))\n",
    "        self.policy = np.zeros(len(self.states_names))\n",
    "        self.delta = 0\n",
    "        self.diff = 10\n",
    "        self.grid = np.ones((4 ,4))\n",
    "        self.values = [0, 0, 0, 0]\n",
    "        self.reward_state = self.dic_states[self.states_names[(cid[7]+1)%3]]\n",
    "        \n",
    "        \n",
    "    def new_states(self):\n",
    "        \"\"\"Figures out if the agent is in a terminal state and computes the potential new states \n",
    "        it could take i.e all its neigbhours. Doing so, it ensures that the agent comes back to its \n",
    "        current position if a transition would lead him outside of the Grid World boundary.\n",
    "        input: self//all the defined parameters\n",
    "        output: a (4,) np.array of 4 (2,) arrays representing the potential new positions it could take.\"\"\"\n",
    "    \n",
    "        self.terminal = 0 #flage to indicate if the current state is terminal \n",
    "        \n",
    "        if (np.all(self.current_state == self.reward_state) or \\\n",
    "            np.all(self.current_state == self.dic_states[\"s11\"])):  #checking if the current position is terminal\n",
    "            self.new_state = self.current_state\n",
    "            self.terminal = 1   #flag to show terminal had been reached\n",
    "            self.pot_new_state = []  #empty list of potential new states as it is static\n",
    "\n",
    "        else:\n",
    "            self.translation = np.array([[-1, 0], [1, 0], [0, 1], [0, -1]])   #Transitions arrays: North, S, E, W\n",
    "            self.pot_new_state = self.current_state + self.translation  #Updating the potential positions\n",
    "            for i in range(len(self.pot_new_state)): \n",
    "                if not((self.pot_new_state[i].tolist() in self.states_pos.tolist())):  #Checking if out of bounds\n",
    "                    self.pot_new_state[i] = self.current_state  #Imposing the current position for out of bounds potential\n",
    "\n",
    "        return (self.pot_new_state, self.terminal)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_rewards(self):\n",
    "        \"\"\"Gathers the value for the rewards for each of the four potential new states in which the agent would \n",
    "        move. Recall that the if the agent is not in a terminal state, the reward is -100 getting to s11, +10 \n",
    "        getting to the positive reward state (here s2), and -1 to any other state in the GridWorld. If it is in a\n",
    "        terminal state, the reward is 0.\"\"\"\n",
    "        self.reward = np.zeros(4)\n",
    "\n",
    "        for i in range(len(self.pot_new_state)):\n",
    "            if self.terminal == 1:    \n",
    "                self.reward[i] = 0    #reward of 0 if in termnial state\n",
    "            else:\n",
    "                if np.all(self.pot_new_state[i] == self.reward_state):\n",
    "                    self.reward[i] = 10    #reward of +10 if getting to the positive termnial state\n",
    "                elif np.all(self.pot_new_state[i] == self.dic_states[\"s11\"]):\n",
    "                    self.reward[i] = -100  #reward of -100 if getting to the negative termnial state\n",
    "                else:\n",
    "                    self.reward[i] = -1    #reward of -1 for any other new state\n",
    "\n",
    "        self.reward = np.reshape(self.reward, (4, 1))\n",
    "        return(self.reward)\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_value(self):\n",
    "        \"\"\"Copmutes the optimal value function at a given state and the policy which derived it. Performs the\n",
    "        calculation of the BOE.\n",
    "        input: self\n",
    "        output: the updated (11,) state value np array and (11,) policy array\"\"\"\n",
    "        self.p = np.ones((4, 4))*self.pbo\n",
    "        \n",
    "        for l in range (self.p.shape[0]):\n",
    "            self.p[l, l] = self.pba   #input the diagonal terms of the probablility matrix to be the p \n",
    "                                      #given by the CID, this is done to perfom the matrix calculation later\n",
    "                                      #with the highest porbability given to the direction which matches the action\n",
    "                                    \n",
    "        if self.terminal == 1:        #all 0s if the state is terminal\n",
    "            self.v[self.s] = 0\n",
    "            self.policy[self.s] = np.nan   \n",
    "        else:\n",
    "            self.values = np.zeros((4, 1))\n",
    "            self.new_state = np.zeros(4)\n",
    "            \n",
    "            for n in range (self.new_state.shape[0]):   #get the index in the state position list \n",
    "                                                        #of each of the potential new states\n",
    "                self.new_state[n] = self.states_pos.tolist().index(self.pot_new_state[n].tolist())\n",
    "                \n",
    "            #gather the present value functions of each of the potential states in a (4, 1) np array\n",
    "            self.mini_values = np.reshape(np.array([self.v[int(self.new_state[0])], self.v[int(self.new_state[1])],\\\n",
    "                                                    self.v[int(self.new_state[2])], self.v[int(self.new_state[3])]]),\\\n",
    "                                          (4, 1)) \n",
    "            self.values = np.matmul(self.p, (self.reward + self.gamma*self.mini_values))  #the 4 potential value fucntions\n",
    "            self.policy[self.s] = np.argmax(np.array(self.values)) #the favored policy 0:North, 1:South, 3:East, 4:West\n",
    "            self.v[self.s] = np.max(self.values) #update the value function with maximum\n",
    "            \n",
    "        return (self.v[self.s], self.policy[self.s])\n",
    "    \n",
    "    def map_policy(self):\n",
    "        \"\"\"Maps the best policy given by numbers with letter names. Acts like a dictionary.\n",
    "        input: self\n",
    "        output: list of length 11, correspopnding to the policy to take at each respective state.\"\"\"\n",
    "        self.policies = [\"north\", \"south\", \"east\", \"west\"]\n",
    "        self.pol = []\n",
    "        for i in range(self.policy.size):\n",
    "            if np.isnan(self.policy[i]):\n",
    "                self.pol.append(\"nan\")  #nan for the terminal states\n",
    "            else:\n",
    "                self.pol.append(self.policies[int(self.policy[i])])\n",
    "        return self.pol\n",
    "    \n",
    "    def grid_form(self):\n",
    "        \"\"\"Displays the grid as a (4x4) array to match the Grid World and clearly represent the value funciton\n",
    "        input: self\n",
    "        output: np (4x4) array with the value function at each state. nan in the blocked positions.\"\"\"\n",
    "        \n",
    "        self.grid = np.nan*np.ones((4,4))\n",
    "        for i in range(len(self.states_names)):\n",
    "            self.x = self.states_pos[i].tolist()[0]\n",
    "            self.y = self.states_pos[i].tolist()[1]\n",
    "            self.grid[self.x, self.y] = self.v[i]\n",
    "        print(self.grid)\n",
    "        \n",
    "        return \n",
    "\n",
    "    def policy_form(self):\n",
    "        \"\"\"Displays the grid as a (4x4) array to match the Grid World and clearly represent the policy\n",
    "        input: self\n",
    "        output: np (4x4) array with the policy at each state. nan in the blocked positions and terminal states.\"\"\"\n",
    "        \n",
    "        l = [6, 8, 12, 13, 15]\n",
    "        for j in range(len(l)):\n",
    "            self.policy_directions.insert(l[j], 'nan')\n",
    "        self.policy_directions = np.reshape(np.asarray(self.policy_directions), (4,4))\n",
    "        print(self.policy_directions)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def value_iteration(self):\n",
    "        \"\"\"Performs the value iteration algorithm up to a given convergence threshold delta.\n",
    "        input: self\n",
    "        output:\"\"\"\n",
    "        \n",
    "        while self.diff > 0.01: \n",
    "            self.v_old = self.v.copy()\n",
    "            for self.s in range(len(self.states_names)):\n",
    "                self.current_state = self.dic_states[self.states_names[self.s]]\n",
    "                (self.pot_new_state, self.terminal) = self.new_states()\n",
    "                self.reward = self.get_rewards()\n",
    "                (self.v[self.s], self.policy[self.s]) = self.get_value()\n",
    "            self.diff = max(self.delta, np.linalg.norm(self.v_old - self.v))\n",
    "            self.policy_directions = self.map_policy()\n",
    "        self.grid_form()\n",
    "        self.policy_form()\n",
    "        return (self.v, self.policy_directions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = Grid_World(np.array([0,1,0,6,8,4,2,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.  0. 10.  9.]\n",
      " [ 9. 10. nan  8.]\n",
      " [nan  9.  8.  7.]\n",
      " [nan nan  0. nan]]\n",
      "[['east' 'nan' 'west' 'west']\n",
      " ['north' 'north' 'nan' 'north']\n",
      " ['nan' 'north' 'west' 'north']\n",
      " ['nan' 'nan' 'nan' 'nan']]\n"
     ]
    }
   ],
   "source": [
    "(final_values, final_policy) = world.value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
