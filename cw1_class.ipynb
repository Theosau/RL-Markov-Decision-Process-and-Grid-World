{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid_World():\n",
    "    \"\"\"This class constructs the Grid World, transition probabililties and \n",
    "       discount factor according to the CID.\n",
    "       It provides th optimal policy and the actual value function states, \n",
    "       based on the value iteration algorithm.\"\"\"\n",
    "    \n",
    "    def __init__(self, cid):\n",
    "        \"\"\"This funciton sets-up the grid as well as the probabililtes, \n",
    "           discount factor, and relevant matrices for the computation of \n",
    "           the value iteration algorithm. It provides the position of all \n",
    "           the unblocked states of the GridWorld of interest.\n",
    "           input: an (8x1) numpy array with each CID digit.\"\"\"\n",
    "        \n",
    "        # Grid World inputs initialisation\n",
    "        self.cid = cid\n",
    "        \n",
    "        # Creating the dictionary of states and values\n",
    "        self.states_names = [\"s\" + str(i+1) for i in range(11)]\n",
    "        self.states_pos = np.array([[0, 0], [0, 1], [0, 2], [0, 3], \n",
    "                                    [1, 0], [1, 1], [1, 3], \n",
    "                                    [2, 1], [2, 2], [2, 3], \n",
    "                                    [3, 2]])\n",
    "        self.dic_states = dict(zip(self.states_names, self.states_pos))\n",
    "        \n",
    "        \n",
    "        # Potential step directions\n",
    "        self.directions = [\"north\", \"south\", \"east\", \"west\"]\n",
    "        \n",
    "        self.gamma = 0.2 + 0.5*0.1*cid[6] \n",
    "        self.pba = 0.25 + 0.5*0.1*cid[5]\n",
    "        self.pbo = (1-self.pba)/3\n",
    "        self.v = np.zeros(len(self.states_names))\n",
    "        self.policy = np.zeros(len(self.states_names))\n",
    "        self.delta = 0\n",
    "        self.diff = 10\n",
    "        self.grid = np.ones((4 ,4))\n",
    "        self.values = [0, 0, 0, 0]\n",
    "        self.reward_state = self.dic_states[self.states_names[(cid[7]+1)%3]]\n",
    "        \n",
    "        \n",
    "    def _new_states(self):\n",
    "        \"\"\"Figures out if the agent is in a terminal state and computes the\n",
    "           potential new states it could take i.e all its neigbhours. Doing \n",
    "           so, it ensures that the agent comes back to its current position \n",
    "           if a transition would lead him outside of the Grid World boundary.\n",
    "           \n",
    "           output: \n",
    "               - self.pot_new_state : a (4,) np.array of 4 (2,) arrays representing \n",
    "                                   the potential new positions it could take.\n",
    "               - self.terminal : bianary flag indicating if the state is terminal.\"\"\"\n",
    "        \n",
    "        # flag to indicate if the current state is terminal \n",
    "        self.terminal = False \n",
    "        \n",
    "        # checking if the current position is terminal\n",
    "        if (np.all(self.current_state == self.reward_state) or \\\n",
    "            np.all(self.current_state == self.dic_states[\"s11\"])):\n",
    "            self.new_state = self.current_state\n",
    "            self.terminal = True # terminal reached\n",
    "            self.pot_new_state = []  #empty list of potential new states as it is static\n",
    "\n",
    "        else:\n",
    "            # transitions arrays: North, S, E, W\n",
    "            self.translation = np.array([[-1, 0], [1, 0], [0, 1], [0, -1]])\n",
    "            # updating the potential positions\n",
    "            self.pot_new_state = self.current_state + self.translation  \n",
    "            \n",
    "            for i in range(len(self.pot_new_state)): \n",
    "                # imposing the current position for out of bounds potential\n",
    "                if not((self.pot_new_state[i].tolist() in self.states_pos.tolist())):\n",
    "                    self.pot_new_state[i] = self.current_state\n",
    "\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def _get_rewards(self):\n",
    "        \"\"\"Gathers the value for the rewards for each of the four potential \n",
    "        new states in which the agent would  move. Recall that the if the \n",
    "        agent is not in a terminal state, the reward is -100 getting to s11, \n",
    "        +10 getting to the positive reward state (here s2), and -1 to any other\n",
    "        state in the GridWorld. If it is in a terminal state, the reward is 0.\"\"\"\n",
    "        \n",
    "        self.reward = np.zeros(4)\n",
    "\n",
    "        for i in range(len(self.pot_new_state)):\n",
    "            if self.terminal:    \n",
    "                self.reward[i] = 0  # 0 reward staying at termnial state\n",
    "            else:\n",
    "                # +10 if reaching the positive termnial state\n",
    "                if np.all(self.pot_new_state[i] == self.reward_state):\n",
    "                    self.reward[i] = 10\n",
    "                \n",
    "                # -100 reaching the negative termnial state\n",
    "                elif np.all(self.pot_new_state[i] == self.dic_states['s11']):\n",
    "                    self.reward[i] = -100\n",
    "                \n",
    "                # -1 for any other state\n",
    "                else:\n",
    "                    self.reward[i] = -1\n",
    "\n",
    "        self.reward = np.reshape(self.reward, (4, 1))\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def _get_value(self):\n",
    "        \"\"\"Copmutes the optimal value function at a given state and \n",
    "           the policy which derived it. Performs the calculation of \n",
    "           the Bellman Optimality Equation.\"\"\"\n",
    "        self.p = np.ones((4, 4))*self.pbo\n",
    "        \n",
    "        \n",
    "        # input the diagonal terms of the probablility matrix to be the p \n",
    "        # given by the CID, this is done to perfom the matrix calculation later\n",
    "        # with the highest porbability given to the direction which matches the action\n",
    "        \n",
    "        for l in range (self.p.shape[0]):\n",
    "            self.p[l, l] = self.pba\n",
    "                         \n",
    "        # all 0s if the state is terminal\n",
    "        if self.terminal:\n",
    "            self.v[self.s] = 0\n",
    "            self.policy[self.s] = np.nan   \n",
    "        else:\n",
    "            self.values = np.zeros((4, 1))\n",
    "            self.new_state = np.zeros(4)\n",
    "            \n",
    "            # get the index in the state position list \n",
    "            # of each of the potential new states\n",
    "            for n in range (self.new_state.shape[0]):\n",
    "                self.new_state[n] = self.states_pos.tolist().index(self.pot_new_state[n].tolist())\n",
    "                \n",
    "            # gather the present value functions of each of \n",
    "            # the potential states in a (4, 1) np array\n",
    "            self.mini_values = np.reshape(np.array([self.v[int(self.new_state[0])], \n",
    "                                                    self.v[int(self.new_state[1])],\n",
    "                                                    self.v[int(self.new_state[2])], \n",
    "                                                    self.v[int(self.new_state[3])]]),\n",
    "                                          (4, 1)) \n",
    "            \n",
    "            # value fucntions in matrix form\n",
    "            self.values = np.matmul(self.p, (self.reward + self.gamma*self.mini_values))\n",
    "            self.policy[self.s] = np.argmax(np.array(self.values))\n",
    "            self.v[self.s] = np.max(self.values)\n",
    "            \n",
    "        return\n",
    "    \n",
    "\n",
    "    def _map_policy(self):\n",
    "        \"\"\"Maps the best policy given by numbers with letter names. \n",
    "           Acts like a dictionary. \"\"\"\n",
    "        self.policies = [\"north\", \"south\", \"east\", \"west\"]\n",
    "        self.policy_directions = []\n",
    "        for i in range(self.policy.size):\n",
    "            if np.isnan(self.policy[i]):\n",
    "                # nan for the terminal states\n",
    "                self.policy_directions.append(\"nan\")\n",
    "            else:\n",
    "                self.policy_directions.append(self.policies[int(self.policy[i])])\n",
    "        return\n",
    "    \n",
    "    def _grid_form(self):\n",
    "        \"\"\"Displays the grid as a (4x4) array to match the Grid World \n",
    "           and clearly represent the value funciton\"\"\"\n",
    "        \n",
    "        self.grid = np.nan*np.ones((4,4))\n",
    "        for i in range(len(self.states_names)):\n",
    "            self.x = self.states_pos[i].tolist()[0]\n",
    "            self.y = self.states_pos[i].tolist()[1]\n",
    "            self.grid[self.x, self.y] = self.v[i]\n",
    "        print(self.grid)\n",
    "        \n",
    "        return \n",
    "\n",
    "\n",
    "    def _policy_form(self):\n",
    "        \"\"\"Displays the grid as a (4x4) array to match the \n",
    "           Grid World and clearly represent the policy\"\"\"\n",
    "        \n",
    "        l = [6, 8, 12, 13, 15]\n",
    "        for j in range(len(l)):\n",
    "            self.policy_directions.insert(l[j], 'nan')\n",
    "        self.policy_directions = np.reshape(np.asarray(self.policy_directions),\n",
    "                                            (4,4))\n",
    "        print(self.policy_directions)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def value_iteration(self):\n",
    "        \"\"\"Performs the value iteration algorithm up to a given \n",
    "           convergence threshold delta.\n",
    "        outputs:\n",
    "            - final_values: np array if shape (11,) with the value \n",
    "              function at each of the states\n",
    "            - final_policy: np array if shape (11,) with the optimal\n",
    "              policy at each of the states\"\"\"\n",
    "        \n",
    "        while self.diff > 0.01: \n",
    "            self.v_old = self.v.copy()\n",
    "            for self.s in range(len(self.states_names)):\n",
    "                self.current_state = self.dic_states[self.states_names[self.s]]\n",
    "                self._new_states()\n",
    "                self._get_rewards()\n",
    "                self._get_value()\n",
    "            self.diff = max(self.delta, np.linalg.norm(self.v_old - self.v))\n",
    "            self._map_policy()\n",
    "\n",
    "        self._grid_form()\n",
    "        self._policy_form()\n",
    "\n",
    "        return (self.v, self.policy_directions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = Grid_World(np.array([0,1,0,6,8,4,2,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.42565843   0.           4.40489154  -0.53964506]\n",
      " [ -0.20091765   4.06478489          nan  -1.3663324 ]\n",
      " [         nan  -1.78377704 -20.6709841   -2.60810627]\n",
      " [         nan          nan   0.                  nan]]\n",
      "[['east' 'nan' 'west' 'west']\n",
      " ['north' 'north' 'nan' 'north']\n",
      " ['nan' 'north' 'west' 'north']\n",
      " ['nan' 'nan' 'nan' 'nan']]\n"
     ]
    }
   ],
   "source": [
    "(final_values, final_policy) = world.value_iteration()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
